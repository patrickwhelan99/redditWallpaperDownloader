#!/bin/bash

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )" # Get the script's current directory


function printUsage {
echo -e "Usage:\n\t-l:\tFilename to take links from\n\t-r:\tSpecify a single subreddit to download from\n\t-q:\tEnables quiet mode\n\t-f:\tUse feh instead of gnome-shell";
}

enableQuiet=false
useFeh=false

while getopts "l:r:qf" o; do
    case "${o}" in
        l)
            linksFile=${OPTARG}
            ;;
	r)
	    singleSub=${OPTARG}
	    ;;
        q)
            enableQuiet=${OPTARG}
            ;;
	f)
	    useFeh=true
	    ;;
        *)
	    printUsage
	    exit -1
            ;;
    esac
done
shift $((OPTIND-1))

if [[ "$linksFile" == "" ]]; then
	linksFile="$DIR/links"
	if [[ $enableQuiet == false && -z $singleSub ]]; then
		echo -e "\n\nAssuming links file is \"links\""
	fi
fi



mkdir $DIR/downloads


function parse {
grep -o -E 'href="([^"#]+)"' $1 | cut -d'"' -f2 | sort | uniq | grep -E '.jpg|.png' >> $DIR/downloads/temp
grep -o -E 'href="([^"#]+)"' $2 | cut -d'"' -f2 | sort | uniq | grep -E '.jpg|.png' >> $DIR/downloads/temp
grep -o -E 'href="([^"#]+)"' $3 | cut -d'"' -f2 | sort | uniq | grep -E '.jpg|.png' >> $DIR/downloads/temp
grep -o -E 'href="([^"#]+)"' $4 | cut -d'"' -f2 | sort | uniq | grep -E '.jpg|.png' >> $DIR/downloads/temp
}

function downloadPages {
rname=$( echo $1 | cut -d / -f 5  )
tname=$(echo t.$rname)
rrname=$(echo r.$rname)
cname=$(echo c.$rname)
wget --load-cookies=$DIR/cookies.txt -O $DIR/downloads/$rname $1  &>/dev/null &
wget --load-cookies=$DIR/cookies.txt -O $DIR/downloads/$tname $1/top &>/dev/null &
wget --load-cookies=$DIR/cookies.txt -O $DIR/downloads/$rrname $1/rising &>/dev/null &
wget --load-cookies=$DIR/cookies.txt -O $DIR/downloads/$cname $1/controversial &>/dev/null &
wait
parse $DIR/downloads/$rname $DIR/downloads/$tname $DIR/downloads/$rrname $DIR/downloads/$cname
}


if [[ $enableQuiet == false && -z $singleSub ]]; then
	echo -e "Subreddits to choose from\n"
	cat $linksFile | grep -v -E "^#" | cut -d\/ -f "5" | column -x
fi

loopCounter=0

# Sometimes reddit doesn't populate the pages quick enough! We need to check there are images to download, if not try again
while [ -z $wallpaper ]; do

	let loopCounter++

	if [[ $loopCounter > 5 ]]; then
		echo -e "Failed to find any images!\nCheck your URLs are correct\nSee https://github.com/patrickwhelan99/redditWallpaperDownloader/blob/master/links for an example"
		rm -r $DIR/downloads
		exit -2;
	fi

	if [ -z $singleSub ]; then
		subRedditChosenFrom="$(shuf -n 1 $linksFile)"
	else
		subRedditChosenFrom="http://www.reddit.com/r/$singleSub"
	fi

	downloadPages "$subRedditChosenFrom"&
	pids+=" "$!""

	wait # wait for all processes to return

	# remove reddit pics that exist on most pages from the list
	sed -i '/www.redditstatic.com\//d' $DIR/downloads/temp
	sed -i '/styles.redditmedia.com\//d' $DIR/downloads/temp
	sed -i '/i.imgur.com\/ZDka3tA.png/d' $DIR/downloads/temp
	sed -i '/i.imgur.com\/vFrZ1C9.png/d' $DIR/downloads/temp
	sed -i '/i.imgur.com\/FqnC5e2.png/d' $DIR/downloads/temp
	sed -i '/i.imgur.com\/4PxSX4e.png/d' $DIR/downloads/temp
	sed -i '/i.redd.it\/jsyu3p9xzlf41.jpg/d' $DIR/downloads/temp

	# select randomly from file and DL
	wallpaper=$(shuf -n 1 $DIR/downloads/temp)

	if [[ $enableQuiet == false && ! -z $wallpaper ]]; then
       		rname=$( echo $subRedditChosenFrom | cut -d / -f 5  )
		echo -e "\nGrabbing image from r/$rname!"
	fi

done

echo -e "From r/$(echo $subRedditChosenFrom | cut -d '/' -f 5)" >> $DIR/log
echo $wallpaper >> $DIR/log
echo -e "\n" >> $DIR/log

wget --load-cookies=$DIR/cookies.txt $wallpaper -O $DIR/wallpaperpic &> /dev/null &
wait

if [[ $useFeh == true || $DESKTOP_SESSION == i3 ]]; then
	feh --bg-fill $DIR/wallpaperpic
else
	gsettings set org.gnome.desktop.background picture-uri file://$DIR/wallpaperpic
	#gsettings set org.gnome.desktop.background picture-options "zoom"
fi

# cleanup
rm -r $DIR/downloads
